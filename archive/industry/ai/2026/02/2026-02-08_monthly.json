{
  "range": {
    "from": "2026-01-10",
    "to": "2026-02-08"
  },
  "kpis": {
    "collected": 42,
    "deduped": 42,
    "uniqueTopics": 49,
    "marketShare": {
      "Tooling": 25,
      "Product": 21,
      "Infra": 20,
      "Models": 20,
      "Research": 13,
      "Other": 1
    }
  },
  "weeklyData": [
    {
      "week": "Week 1 (01/10-01/16)",
      "topicCounts": {
        "Tooling": 0,
        "Product": 0,
        "Infra": 0,
        "Models": 0,
        "Research": 0
      }
    },
    {
      "week": "Week 2 (01/17-01/23)",
      "topicCounts": {
        "Tooling": 0,
        "Product": 0,
        "Infra": 0,
        "Models": 0,
        "Research": 0
      }
    },
    {
      "week": "Week 3 (01/24-01/30)",
      "topicCounts": {
        "Tooling": 7,
        "Product": 7,
        "Infra": 5,
        "Models": 6,
        "Research": 6
      }
    },
    {
      "week": "Week 4 (01/31-02/06)",
      "topicCounts": {
        "Tooling": 12,
        "Product": 10,
        "Infra": 10,
        "Models": 10,
        "Research": 5
      }
    },
    {
      "week": "Week 5 (02/07-02/08)",
      "topicCounts": {
        "Tooling": 0,
        "Product": 0,
        "Infra": 0,
        "Models": 0,
        "Research": 0
      }
    }
  ],
  "topIssues": [
    {
      "id": 1,
      "status": "NEW",
      "title": "초저정밀 포맷 NVFP4로 AI 학습·추론 가속",
      "summary": "NVIDIA는 NVFP4 초저정밀 부동소수점 포맷과 전용 하드웨어·소프트웨어 최적화를 통해 연산 처리량을 늘리고 메모리·대역폭 사용을 줄여 대규모 모델의 학습과 추론을 크게 가속화하고 있다. 기존 ML 프레임워크와 호환되도록 설계해 도입 장벽을 낮춘 것이 핵심이다.",
      "articleCount": 1,
      "relatedArticles": [
        {
          "source": "NVIDIA Developer Blog",
          "title": "3 Ways NVFP4 Accelerates AI Training and Inference",
          "url": "https://developer.nvidia.com/blog/3-ways-nvfp4-accelerates-ai-training-and-inference/"
        }
      ]
    },
    {
      "id": 2,
      "status": "ONGOING",
      "title": "글로벌 AI 모델의 지역화 전략과 안전성 유지",
      "summary": "OpenAI는 전 세계 사용자에게 동일한 최첨단 모델을 제공하되, 각 지역의 언어·법규·문화에 맞게 응답을 조정하는 지역화 전략을 제시했다. 접근성 확장과 규제 준수, 문화적 적합성을 확보하면서도 안전성과 기본 정책 일관성을 유지하는 것이 장기 과제로 이어지고 있다.",
      "articleCount": 1,
      "relatedArticles": [
        {
          "source": "OpenAI Blog",
          "title": "Making AI work for everyone, everywhere: our approach to localization",
          "url": "https://openai.com/index/our-approach-to-localization"
        }
      ]
    },
    {
      "id": 3,
      "status": "NEW",
      "title": "한국 법규에 맞춘 OpenAI 전용 개인정보 처리방침 공개",
      "summary": "OpenAI는 한국 사용자를 위한 별도 개인정보 처리방침을 발표해 국내 법령에 부합하는 데이터 수집·이용·보관·제3자 제공 기준과 이용자 권리 행사를 명확히 했다. 글로벌 AI 서비스의 현지 규제 정합성을 높이는 움직임으로 해석된다.",
      "articleCount": 1,
      "relatedArticles": [
        {
          "source": "OpenAI Blog",
          "title": "Korea privacy policy",
          "url": "https://openai.com/policies/kr-privacy-policy"
        }
      ]
    },
    {
      "id": 4,
      "status": "ONGOING",
      "title": "게임 자산 현대화를 위한 생성형 AI 제작 파이프라인",
      "summary": "NVIDIA는 Painkiller RTX 사례를 통해 업스케일링, 자동 텍스처 생성 등 생성형 AI 기반 워크플로우로 기존 게임 자산을 대규모로 현대화하는 방법을 제시했다. 소규모 팀도 고해상도 그래픽을 효율적으로 제작·갱신할 수 있는 실무형 파이프라인이 확산되고 있다.",
      "articleCount": 1,
      "relatedArticles": [
        {
          "source": "NVIDIA Developer Blog",
          "title": "How Painkiller RTX Uses Generative AI to Modernize Game Assets at Scale",
          "url": "https://developer.nvidia.com/blog/how-painkiller-rtx-uses-generative-ai-to-modernize-game-assets-at-scale/"
        }
      ]
    },
    {
      "id": 5,
      "status": "NEW",
      "title": "Nemotron 기반 고급 RAG 문서 처리 파이프라인",
      "summary": "NVIDIA는 Nemotron을 활용해 복잡한 PDF, 중첩 표, 차트 내 데이터 등 비정형·반정형 문서를 정교하게 처리하는 RAG 전용 파이프라인 구축 방법을 공개했다. 고품질 인덱싱과 추출을 통해 엔터프라이즈 문서 검색·질의 응답 시스템의 정확도와 활용성을 높이는 데 초점을 맞춘다.",
      "articleCount": 1,
      "relatedArticles": [
        {
          "source": "NVIDIA Developer Blog",
          "title": "How to Build a Document Processing Pipeline for RAG with Nemotron",
          "url": "https://developer.nvidia.com/blog/how-to-build-a-document-processing-pipeline-for-rag-with-nemotron/"
        }
      ]
    }
  ]
}