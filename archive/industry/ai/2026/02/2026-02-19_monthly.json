{
  "range": {
    "from": "2026-01-21",
    "to": "2026-02-19"
  },
  "kpis": {
    "collected": 62,
    "deduped": 62,
    "uniqueTopics": 49,
    "marketShare": {
      "Tooling": 25,
      "Product": 23,
      "Infra": 20,
      "Models": 18,
      "Research": 14,
      "Other": 0
    }
  },
  "weeklyData": [
    {
      "week": "Week 1 (01/21-01/27)",
      "topicCounts": {
        "Tooling": 0,
        "Product": 0,
        "Infra": 0,
        "Models": 0,
        "Research": 0
      }
    },
    {
      "week": "Week 2 (01/28-02/03)",
      "topicCounts": {
        "Tooling": 8,
        "Product": 7,
        "Infra": 7,
        "Models": 5,
        "Research": 5
      }
    },
    {
      "week": "Week 3 (02/04-02/10)",
      "topicCounts": {
        "Tooling": 12,
        "Product": 13,
        "Infra": 10,
        "Models": 7,
        "Research": 4
      }
    },
    {
      "week": "Week 4 (02/11-02/17)",
      "topicCounts": {
        "Tooling": 4,
        "Product": 6,
        "Infra": 1,
        "Models": 5,
        "Research": 4
      }
    },
    {
      "week": "Week 5 (02/18-02/19)",
      "topicCounts": {
        "Tooling": 3,
        "Product": 2,
        "Infra": 4,
        "Models": 1,
        "Research": 2
      }
    }
  ],
  "topIssues": [
    {
      "id": "ai-1",
      "status": "NEW",
      "title": "GPU 분할과 새 정밀도 포맷으로 가속되는 차세대 LLM 인프라",
      "summary": "NVIDIA가 GPU Fractioning과 NVFP4 초저정밀 포맷을 통해 같은 하드웨어에서 더 많은 토큰을 처리하고 학습·추론 비용을 낮추는 전략을 제시했다. 단일 GPU를 여러 워커로 쪼개 예측 가능한 지연 시간으로 동시 추론을 늘리고, 4비트 수준의 연산·메모리 최적화로 처리량과 에너지 효율을 극대화해 대규모 LLM 서비스 인프라의 성능·단가 구조를 동시에 개선하는 흐름이 강화되고 있다.",
      "articleCount": 2,
      "relatedArticles": [
        {
          "source": "NVIDIA Developer Blog",
          "title": "Unlock Massive Token Throughput with GPU Fractioning in NVIDIA Run:ai",
          "url": "https://developer.nvidia.com/blog/unlock-massive-token-throughput-with-gpu-fractioning-in-nvidia-runai/"
        },
        {
          "source": "NVIDIA Developer Blog",
          "title": "3 Ways NVFP4 Accelerates AI Training and Inference",
          "url": "https://developer.nvidia.com/blog/3-ways-nvfp4-accelerates-ai-training-and-inference/"
        }
      ]
    },
    {
      "id": "ai-2",
      "status": "NEW",
      "title": "파이썬 기반 GPU 커널과 멀티모달 RAG로 진화하는 AI 개발 스택",
      "summary": "NVIDIA는 cuda.compute를 통해 파이썬에서 고성능 GPU 커널을 작성할 수 있게 하고, 멀티모달 RAG의 5대 핵심 역량을 정리해 텍스트·표·차트·이미지·스캔 문서를 아우르는 기업용 지식 시스템 구축 방법을 제안했다. 고성능 커널 개발의 진입 장벽을 낮추면서도, 검색·생성 파이프라인 전반을 멀티모달로 통합하려는 시도가 맞물리며 AI 애플리케이션 개발 스택이 생산성과 확장성을 동시에 추구하는 방향으로 재편되고 있다.",
      "articleCount": 2,
      "relatedArticles": [
        {
          "source": "NVIDIA Developer Blog",
          "title": "Topping the GPU MODE Kernel Leaderboard with NVIDIA cuda.compute",
          "url": "https://developer.nvidia.com/blog/topping-the-gpu-mode-kernel-leaderboard-with-nvidia-cuda-compute/"
        },
        {
          "source": "NVIDIA Developer Blog",
          "title": "Build AI-Ready Knowledge Systems Using 5 Essential Multimodal RAG Capabilities",
          "url": "https://developer.nvidia.com/blog/build-ai-ready-knowledge-systems-using-5-essential-multimodal-rag-capabilities/"
        }
      ]
    },
    {
      "id": "ai-3",
      "status": "NEW",
      "title": "소버린 AI를 겨냥한 소형 특화 언어모델의 확산",
      "summary": "NVIDIA의 일본어 특화 9B 파라미터 모델 Nemotron 2 Nano 9B Japanese는 경량·고효율 구조로 로컬 배포와 저지연 서비스를 지원하며 일본의 소버린 AI 전략을 뒷받침한다. 초거대 범용 모델 대신, 국가·언어·도메인에 특화된 중·소형 모델을 통해 데이터 주권과 규제 준수, 비용 최적화를 동시에 달성하려는 소버린 AI 흐름이 구체적인 제품 형태로 전개되고 있다.",
      "articleCount": 1,
      "relatedArticles": [
        {
          "source": "Hugging Face Blog",
          "title": "NVIDIA Nemotron 2 Nano 9B Japanese: 日本のソブリンAIを支える最先端小規模言語モデル",
          "url": "https://huggingface.co/blog/nvidia/nemotron-nano-9b-v2-japanese-ja"
        }
      ]
    },
    {
      "id": "ai-4",
      "status": "NEW",
      "title": "AI 보안을 강화하는 ChatGPT Lockdown Mode와 국방용 배포",
      "summary": "OpenAI는 ChatGPT에 Lockdown Mode와 Elevated Risk 레이블을 도입해 프롬프트 인젝션과 데이터 유출 징후를 탐지·억제하고, 미 국방 네트워크 GenAI.mil에 특화 ChatGPT를 배포해 고보안 환경에서의 활용을 확대하고 있다. 상용 LLM 서비스가 보안 설정, 위험 레이블링, 폐쇄망 배포 지원 등을 통해 공공·국방·규제 산업으로 확장되는 동시에, AI 사용 자체를 보안 통제 대상으로 다루는 추세가 두드러지고 있다.",
      "articleCount": 2,
      "relatedArticles": [
        {
          "source": "OpenAI Blog",
          "title": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT",
          "url": "https://openai.com/index/introducing-lockdown-mode-and-elevated-risk-labels-in-chatgpt"
        },
        {
          "source": "OpenAI Blog",
          "title": "Bringing ChatGPT to GenAI.mil",
          "url": "https://openai.com/index/bringing-chatgpt-to-genaimil"
        }
      ]
    },
    {
      "id": "ai-5",
      "status": "NEW",
      "title": "차세대 AI 인프라를 겨냥한 초장기 유리 데이터 스토리지",
      "summary": "Microsoft의 Project Silica는 붕규산 유리에 데이터를 인코딩해 10,000년 보존을 목표로 하는 차세대 스토리지를 제시했다. 미디어 비용을 낮추고 쓰기·읽기 시스템을 단순화하는 이 기술은 대규모 AI 학습용 데이터 레이크와 규제 데이터 아카이브를 초장기적으로 보존하려는 클라우드·하이퍼스케일러 전략의 핵심 인프라 후보로 부상하고 있다.",
      "articleCount": 1,
      "relatedArticles": [
        {
          "source": "Microsoft Research Blog",
          "title": "Project Silica’s advances in glass storage technology",
          "url": "https://www.microsoft.com/en-us/research/blog/project-silicas-advances-in-glass-storage-technology/"
        }
      ]
    }
  ]
}