name: Crawl briefing

on:
  schedule:
    - cron: "0 22 * * *"
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: data-pipelines-${{ github.ref }}
  cancel-in-progress: false

jobs:
  check:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Check OpenAI connection
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: python3 -m scripts.check_openai_connection

  briefing:
    needs: check
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run briefing pipeline
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: python3 -m scripts.run_briefing_pipeline

      - name: Prune old briefing archives (keep 90d)
        run: |
          python3 - <<'PY'
          from __future__ import annotations

          from datetime import date, datetime, timedelta
          from pathlib import Path
          from zoneinfo import ZoneInfo

          KEEP_DAYS = 90
          TZ = ZoneInfo("Asia/Seoul")
          today_kst = datetime.now(TZ).date()
          cutoff = today_kst - timedelta(days=KEEP_DAYS)

          tabs = ["ai", "finance", "semiconductor", "ev"]
          archive_root = Path("archive")

          deleted = 0
          scanned = 0

          for tab in tabs:
            tab_root = archive_root / tab
            if not tab_root.exists():
              continue

            for path in tab_root.rglob("*_*.json"):
              if not path.is_file():
                continue
              scanned += 1

              stem = path.name
              if "_" not in stem:
                continue
              date_part = stem.split("_", 1)[0]
              try:
                file_date = date.fromisoformat(date_part)
              except ValueError:
                continue

              if file_date < cutoff:
                path.unlink(missing_ok=True)
                deleted += 1

          # Remove empty year/month directories after deletions.
          dirs = []
          for tab in tabs:
            tab_root = archive_root / tab
            if not tab_root.exists():
              continue
            for d in tab_root.rglob("*"):
              if d.is_dir():
                dirs.append(d)

          for d in sorted(dirs, key=lambda p: len(p.parts), reverse=True):
            try:
              next(d.iterdir())
            except StopIteration:
              d.rmdir()

          print(f"[prune] keep_days={KEEP_DAYS} today_kst={today_kst} cutoff={cutoff} scanned={scanned} deleted={deleted}")
          PY

      - name: Commit updates
        run: |
          git status --porcelain
          if [ -n "$(git status --porcelain)" ]; then
            git add public/briefing archive
            git -c user.name="github-actions[bot]" -c user.email="github-actions[bot]@users.noreply.github.com" commit -m "Update briefing data $(TZ=Asia/Seoul date +'%Y-%m-%d')"
            for attempt in 1 2 3; do
              git fetch origin main
              if git pull --rebase origin main && git push; then
                break
              fi
              echo "Push failed (attempt $attempt/3); retrying..."
              sleep $((attempt * 5))
            done
          fi
